{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "from model import Net, CNNModel, MLP\n",
    "from model import *\n",
    "import torch.optim as optim\n",
    "import torch\n",
    "import torchvision\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from sklearn.model_selection import train_test_split\n",
    "from os import walk\n",
    "from tqdm import tqdm\n",
    "\n",
    "from data_parser import get_torch_tensors\n",
    "from sklearn.metrics import r2_score\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Generate torch data\n",
    "# root = '/l/users/20020038/poro_files/'\n",
    "# train_x, train_y_poro, train_y_perm, test_x, test_y_poro, test_y_perm = get_torch_tensors(root, xyz_splits=[100,100,100], size=10)\n",
    "\n",
    "# print(train_x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Select torch data\n",
    "# k = np.random.randint(0, 2000000, 100000)\n",
    "# train_x_sample = train_x[k]\n",
    "# train_y_poro_sample = train_y_poro[k]\n",
    "# train_y_perm_sample = train_y_perm[k]\n",
    "# test_x_sample = test_x[k]\n",
    "# test_y_poro_sample = test_y_poro[k]\n",
    "# test_y_perm_sample = test_y_perm[k]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "root = '/l/users/20020038/poro_files/'\n",
    "# # Save data\n",
    "# torch.save(train_x_sample, f'{root}saved_tensors/train_x_sample.pt')\n",
    "# torch.save(train_y_poro_sample, f'{root}saved_tensors/train_y_poro_sample.pt')\n",
    "# torch.save(train_y_perm_sample, f'{root}saved_tensors/train_y_perm_sample.pt')\n",
    "# torch.save(test_x_sample, f'{root}saved_tensors/test_x_sample.pt')\n",
    "# torch.save(test_y_poro_sample, f'{root}saved_tensors/test_y_poro_sample.pt')\n",
    "# torch.save(test_y_perm_sample, f'{root}saved_tensors/test_y_perm_sample.pt')\n",
    "\n",
    "# Load data\n",
    "train_x_sample = torch.load(f'{root}saved_tensors/train_x_sample.pt')\n",
    "train_y_poro_sample = torch.load(f'{root}saved_tensors/train_y_poro_sample.pt')\n",
    "train_y_perm_sample = torch.load(f'{root}saved_tensors/train_y_perm_sample.pt')\n",
    "test_x_sample = torch.load(f'{root}saved_tensors/test_x_sample.pt')\n",
    "test_y_poro_sample = torch.load(f'{root}saved_tensors/test_y_poro_sample.pt')\n",
    "test_y_perm_sample = torch.load(f'{root}saved_tensors/test_y_perm_sample.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selected indices\n",
    "k = 100000\n",
    "# train_inp, train_out = train_in[:k], train_o[:k]\n",
    "# test_inp, test_out = test_in[:k], test_o[:k]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch_size=128\n",
    "\n",
    "# train = torch.utils.data.TensorDataset(train_inp, train_out)\n",
    "# test = torch.utils.data.TensorDataset(test_inp, test_out)\n",
    "\n",
    "# train_loader = torch.utils.data.DataLoader(train, batch_size=batch_size, shuffle=False)\n",
    "# test_loader = torch.utils.data.DataLoader(test, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# print('number iteration', int(k/batch_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataloader from tensors\n",
    "batch_size=128\n",
    "\n",
    "train_out = torch.zeros((k, 1))\n",
    "train_out[:,0] = train_y_poro_sample\n",
    "# train_out[:,0] = train_y_perm_sample\n",
    "\n",
    "test_out = torch.zeros((k, 1))\n",
    "test_out[:,0] = test_y_poro_sample\n",
    "# test_out[:,0] = test_y_perm_sample\n",
    "\n",
    "train_inp = train_x_sample\n",
    "test_inp = test_x_sample\n",
    "\n",
    "train = torch.utils.data.TensorDataset(train_inp, train_out)\n",
    "test = torch.utils.data.TensorDataset(test_inp, test_out)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train, batch_size=batch_size, shuffle=False)\n",
    "test_loader = torch.utils.data.DataLoader(test, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "print('number iteration', int(k/batch_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_inp.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shape = train_inp.shape\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "# net = MLP(1000).to(device) # select mlp model\n",
    "# net = CNNModel().to(device) # select cnn model\n",
    "net = Net(shape[1:], batch_size).to(device) # select transformer model\n",
    "train_losses = []\n",
    "test_losses = []\n",
    "train_r2 = []\n",
    "test_r2 = []\n",
    "\n",
    "# Training setup\n",
    "lr = 0.00001\n",
    "epochs = 100\n",
    "criterion = torch.nn.MSELoss()\n",
    "optimizer = optim.SGD(net.parameters(), lr=lr, momentum=0.9)\n",
    "\n",
    "# First train loss \n",
    "running_loss = 0.0\n",
    "for i, data in tqdm(enumerate(train_loader, 0)):\n",
    "\n",
    "    inputs, labels = data\n",
    "    # inputs = inputs.unsqueeze(1)\n",
    "    inputs, labels = inputs.to(device), labels.to(device)\n",
    "    outputs = net(inputs)\n",
    "    loss = criterion(outputs, labels)\n",
    "    optimizer.step()\n",
    "\n",
    "    running_loss += loss.item()\n",
    "\n",
    "train_losses.append(running_loss/len(train_loader))\n",
    "o, l = outputs.cpu().detach().numpy().reshape(-1,1), labels.cpu().detach().numpy().reshape(-1,1)\n",
    "r2 = r2_score(l, o)\n",
    "train_r2.append(r2)\n",
    "print('train error', running_loss/i)\n",
    "\n",
    "# First validation loss\n",
    "running_loss = 0.0\n",
    "for i, data in tqdm(enumerate(test_loader, 0)):\n",
    "\n",
    "    inputs, labels = data\n",
    "    # inputs = inputs.unsqueeze(1)\n",
    "    inputs, labels = inputs.to(device), labels.to(device)\n",
    "    outputs = net(inputs)\n",
    "    loss = criterion(outputs, labels)\n",
    "\n",
    "    running_loss += loss.item()\n",
    "\n",
    "test_losses.append(running_loss/len(test_loader))\n",
    "o, l = outputs.cpu().detach().numpy().reshape(-1,1), labels.cpu().detach().numpy().reshape(-1,1)\n",
    "r2 = r2_score(l, o)\n",
    "test_r2.append(r2)\n",
    "print('test error', running_loss/i)\n",
    "\n",
    "# Train model\n",
    "for epoch in range(epochs):\n",
    " \n",
    "    running_loss = 0.0\n",
    "    \n",
    "    for i, data in tqdm(enumerate(train_loader, 0)):\n",
    "        \n",
    "        inputs, labels = data\n",
    "        # inputs = inputs.unsqueeze(1)\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "        \n",
    "    train_losses.append(running_loss/len(train_loader))\n",
    "    o, l = outputs.cpu().detach().numpy().reshape(-1,1), labels.cpu().detach().numpy().reshape(-1,1)\n",
    "    r2 = r2_score(l, o)\n",
    "    train_r2.append(r2)\n",
    "    \n",
    "    if epoch % 10 == 0:\n",
    "        print('epoch', epoch)\n",
    "        print('train error', running_loss/i)\n",
    "    \n",
    "    running_loss = 0.0\n",
    "    \n",
    "    for i, data in tqdm(enumerate(test_loader, 0)):\n",
    "        \n",
    "        inputs, labels = data\n",
    "        # inputs = inputs.unsqueeze(1)\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        \n",
    "    test_losses.append(running_loss/len(test_loader))\n",
    "    o, l = outputs.cpu().detach().numpy().reshape(-1,1), labels.cpu().detach().numpy().reshape(-1,1)\n",
    "    r2 = r2_score(l, o)\n",
    "    test_r2.append(r2)\n",
    "    \n",
    "    if epoch % 10 == 0:\n",
    "        print('test error', running_loss/i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_outputs = []\n",
    "# all_labels = []\n",
    "\n",
    "# for i, data in tqdm(enumerate(train_loader, 0)):\n",
    "\n",
    "#     inputs, labels = data\n",
    "#     # inputs = inputs.unsqueeze(1)\n",
    "#     inputs, labels = inputs.to(device), labels.to(device)\n",
    "#     outputs = net(inputs)\n",
    "#     loss = criterion(outputs, labels)\n",
    "#     optimizer.step()\n",
    "\n",
    "#     o, l = outputs.cpu().detach().numpy().reshape(-1,1), labels.cpu().detach().numpy().reshape(-1,1)\n",
    "#     all_outputs.append(o)\n",
    "#     all_labels.append(l)\n",
    "#     # print('outputs', outputs.flatten())\n",
    "#     # print('labels', labels.flatten())\n",
    "#     # break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# r2_score(np.concatenate(all_labels).flatten(), np.concatenate(all_outputs).flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_y_poro_sample mean = 0.7; max = 1; min = 0; std = 0.4\n",
    "# test_y_perm_sample mean = 0.7; max = 1; min = 0; std = 0.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_losses = np.array(train_losses)\n",
    "test_losses = np.array(test_losses)\n",
    "train_r2 = np.array(train_r2)\n",
    "test_r2 = np.array(test_r2)\n",
    "\n",
    "np.save('logs/' + 'train_losses', train_losses)\n",
    "np.save('logs/' + 'test_losses', test_losses)\n",
    "np.save('logs/' + 'train_r2', train_r2)\n",
    "np.save('logs/' + 'test_r2', test_r2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save data\n",
    "def draw(array, label, title):\n",
    "    epochs = range(1,len(array)+1)\n",
    "    plt.plot(epochs, array, 'g', label=label)\n",
    "    plt.title(title)\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.savefig('logs/' + label + '.pdf')\n",
    "    plt.close()\n",
    "    \n",
    "    \n",
    "draw(train_losses, 'Training Loss', 'Training Loss')\n",
    "draw(test_losses, 'Test Loss', 'Test Loss')\n",
    "draw(train_r2, 'Training R2', 'Training R2')\n",
    "draw(test_r2, 'Test R2', 'Test R2')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "544daac24cddf8e4e7c55e32b526307917a7aabc4423360c0a642a0a34736377"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
